{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. Create a new file named model_evaluation.py or model_evaluation.ipynb for these exercises.\n",
    "\n",
    "- 2. Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "<pre>\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. In the context of this problem, what is a false positive?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking for cats, predict cat, but get dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. In the context of this problem, what is a false negative?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking for cats, predict dog, but get cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. How would you describe this model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without doing any math and just observing, the model performed very well because the true positive and true negative values are much higher than the false values.\n",
    "\n",
    "# Looking for cats:\n",
    "\n",
    "tp = 34\n",
    "fp = 7\n",
    "tn = 46\n",
    "fn = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.8\n",
      "Recall is 0.72\n",
      "Precision is 0.83\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(\"Accuracy is\", accuracy)\n",
    "print(\"Recall is\", round(recall,2))\n",
    "print(\"Precision is\", round(precision,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found here.\n",
    "\n",
    "Use the predictions dataset and pandas to help answer the following questions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.metrics as sk\n",
    "\n",
    "c3 = pd.read_csv('c3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- Recall metric. In this case, the internal team wants to identify as many of the ducks that have a defect as possible. This means that the team is more concerned with minimizing false negatives.\n",
    "\n",
    "- Model 3 has the highest recall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual\n",
       "No Defect    184\n",
       "Defect        16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3['actual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baseline\n",
       "Defect    200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3['baseline'] = 'Defect'\n",
    "\n",
    "c3['baseline'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for actual: 1.0\n",
      "Recall for model1: 0.5\n",
      "Recall for model2: 0.5625\n",
      "Recall for model3: 0.8125\n",
      "Recall for baseline: 1.0\n"
     ]
    }
   ],
   "source": [
    "for col in c3:\n",
    "    score = sk.recall_score(c3['actual'], c3[col], pos_label='Defect')\n",
    "    print(f'Recall for {col}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model1 recall: 50.00%\n",
      "   model2 recall: 56.25%\n",
      "   model3 recall: 81.25%\n",
      "baseline recall: 100.00%\n"
     ]
    }
   ],
   "source": [
    "subset = c3[c3.actual == 'Defect']\n",
    "\n",
    "model1_recall = (subset.model1 == subset.actual).mean()\n",
    "model2_recall = (subset.model2 == subset.actual).mean()\n",
    "model3_recall = (subset.model3 == subset.actual).mean()\n",
    "baseline_recall = (subset.baseline == subset.actual).mean()\n",
    "\n",
    "print(f'   model1 recall: {model1_recall:.2%}')\n",
    "print(f'   model2 recall: {model2_recall:.2%}')\n",
    "print(f'   model3 recall: {model3_recall:.2%}')\n",
    "print(f'baseline recall: {baseline_recall:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- Precision metric. Given the high stakes of giving out a false negative, we would want to prioritize those. Also due to the high stakes, the amount of false positives would be ok.\n",
    "\n",
    "- Model 1 has the highest precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for actual: 1.0\n",
      "Precision for model1: 0.8\n",
      "Precision for model2: 0.1\n",
      "Precision for model3: 0.13131313131313133\n",
      "Precision for baseline: 0.08\n"
     ]
    }
   ],
   "source": [
    "# Using SkLearn\n",
    "\n",
    "for col in c3:\n",
    "    score = sk.precision_score(c3['actual'], c3[col], pos_label='Defect')\n",
    "    print(f'Precision for {col}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Precision: 100.00%\n",
      "Baseline: 100.00%\n",
      "\n",
      "Model Precision: 80.00%\n",
      "Baseline: 80.00%\n",
      "\n",
      "Model Precision: 10.00%\n",
      "Baseline: 10.00%\n",
      "\n",
      "Model Precision: 13.13%\n",
      "Baseline: 13.13%\n",
      "\n",
      "Model Precision: 8.00%\n",
      "Baseline: 8.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using math\n",
    "\n",
    "for col in c3:        \n",
    "    subset = c3[c3[col] == 'Defect']\n",
    "    model_precision = (subset[col] == subset.actual).mean()\n",
    "\n",
    "    subset = c3[c3[col] == 'Defect']\n",
    "    baseline_precision = (subset[col] == subset.actual).mean()\n",
    "\n",
    "    print(f'Model Precision: {model_precision:.2%}')\n",
    "    print(f'Baseline: {baseline_precision:.2%}')\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. You are working as a data scientist for Gives You Paws â„¢, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. \n",
    "\n",
    "- Phase I: An automated algorithm tags pictures as either a cat or a dog.\n",
    "\n",
    "- Phase II: Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users\n",
    "\n",
    "Several models have already been developed with the data, and you can find their results here.\n",
    "\n",
    "Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.metrics as sk\n",
    "\n",
    "paws = pd.read_csv('paws.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual\n",
       "dog    3254\n",
       "cat    1746\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws['actual'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "paws['baseline'] = 'dog'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['model1', 0.8074],\n",
       " ['model2', 0.6304],\n",
       " ['model3', 0.5096],\n",
       " ['model4', 0.7426],\n",
       " ['baseline', 0.6508]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_acc = []\n",
    "\n",
    "for model in paws.columns[1:]:\n",
    "    acc = (paws.actual == paws[model]).mean()\n",
    "    model_acc.append([model, acc])\n",
    "\n",
    "model_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- Model 1 has the highest accuracy score, then model 4. The other two are below baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for actual: 1.0\n",
      "Accuracy for model1: 0.8074\n",
      "Accuracy for model2: 0.6304\n",
      "Accuracy for model3: 0.5096\n",
      "Accuracy for model4: 0.7426\n",
      "Accuracy for baseline: 0.6508\n"
     ]
    }
   ],
   "source": [
    "for col in paws:\n",
    "    score = sk.accuracy_score(paws['actual'], paws[col])\n",
    "    print(f'Accuracy for {col}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['model1', 0.8900238338440586],\n",
       " ['model2', 0.8931767337807607],\n",
       " ['model3', 0.6598883572567783],\n",
       " ['model4', 0.7312485304490948],\n",
       " ['baseline', 0.6508]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pre = []\n",
    "\n",
    "for model in paws.columns[1:]:\n",
    "    \n",
    "    subset = paws [paws[model] == 'dog']\n",
    "    \n",
    "    precision = (subset.actual == subset[model]).mean()\n",
    "\n",
    "    model_pre.append([model,precision])\n",
    "    \n",
    "model_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recommend?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    " - Model 4. When we change the 'actual' to dogs only to represent the team that only deals with dog pictures, it has a 95% accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for actual: 1.0\n",
      "Accuracy for model1: 0.803318992009834\n",
      "Accuracy for model2: 0.49078057775046097\n",
      "Accuracy for model3: 0.5086047940995697\n",
      "Accuracy for model4: 0.9557467732022127\n",
      "Accuracy for baseline: 1.0\n"
     ]
    }
   ],
   "source": [
    "paws_dog_only = paws[paws['actual'] == 'dog']\n",
    "\n",
    "for col in paws_dog_only:\n",
    "    score = sk.accuracy_score(paws_dog_only['actual'], paws_dog_only[col])\n",
    "    print(f'Accuracy for {col}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for actual: 1.0\n",
      "F1 Score for model1: 0.89093387866394\n",
      "F1 Score for model2: 0.6584209441352298\n",
      "F1 Score for model3: 0.6742717457730698\n",
      "F1 Score for model4: 0.9773727215587681\n",
      "F1 Score for baseline: 1.0\n"
     ]
    }
   ],
   "source": [
    "for col in paws_dog_only:\n",
    "    score = sk.f1_score(paws_dog_only['actual'], paws_dog_only[col], pos_label='dog')\n",
    "    print(f'F1 Score for {col}: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recommend?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    " - Model 2. When we change the 'actual' to cats only to represent the team that only deals with cat pictures, it has an 89% accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for actual: 1.0\n",
      "Accuracy for model1: 0.8150057273768614\n",
      "Accuracy for model2: 0.8906071019473081\n",
      "Accuracy for model3: 0.5114547537227949\n",
      "Accuracy for model4: 0.34536082474226804\n",
      "Accuracy for baseline: 0.0\n"
     ]
    }
   ],
   "source": [
    "paws_cat_only = paws[paws['actual'] == 'cat']\n",
    "\n",
    "for col in paws_cat_only:\n",
    "    score = sk.accuracy_score(paws_cat_only['actual'], paws_cat_only[col])\n",
    "    print(f'Accuracy for {col}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(df):\n",
    "    baseline_recall = (subset[col] == subset['actual']).mean()\n",
    "    print(f'Baseline Recall: {baseline_recall}')\n",
    "    for col in df:\n",
    "        score = (df[col] == df['actual']).mean()\n",
    "        print(f'Accuracy for {col}: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Follow the links below to read the documentation about each function, then apply those functions to the data from the previous problem.\n",
    "\n",
    "- sklearn.metrics.accuracy_score\n",
    "- sklearn.metrics.precision_score\n",
    "- sklearn.metrics.recall_score\n",
    "- sklearn.metrics.classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.890024</td>\n",
       "      <td>0.8074</td>\n",
       "      <td>0.789898</td>\n",
       "      <td>0.820096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.815006</td>\n",
       "      <td>0.803319</td>\n",
       "      <td>0.8074</td>\n",
       "      <td>0.809162</td>\n",
       "      <td>0.807400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.747178</td>\n",
       "      <td>0.844452</td>\n",
       "      <td>0.8074</td>\n",
       "      <td>0.795815</td>\n",
       "      <td>0.810484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>1746.000000</td>\n",
       "      <td>3254.000000</td>\n",
       "      <td>0.8074</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   cat          dog  accuracy    macro avg  weighted avg\n",
       "precision     0.689772     0.890024    0.8074     0.789898      0.820096\n",
       "recall        0.815006     0.803319    0.8074     0.809162      0.807400\n",
       "f1-score      0.747178     0.844452    0.8074     0.795815      0.810484\n",
       "support    1746.000000  3254.000000    0.8074  5000.000000   5000.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model 1\")\n",
    "pd.DataFrame(classification_report(paws.actual, paws.model1, \n",
    "                      labels=['cat','dog'],\n",
    "                      output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.893177</td>\n",
       "      <td>0.6304</td>\n",
       "      <td>0.688649</td>\n",
       "      <td>0.750335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.890607</td>\n",
       "      <td>0.490781</td>\n",
       "      <td>0.6304</td>\n",
       "      <td>0.690694</td>\n",
       "      <td>0.630400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.627269</td>\n",
       "      <td>0.633479</td>\n",
       "      <td>0.6304</td>\n",
       "      <td>0.630374</td>\n",
       "      <td>0.631310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>1746.000000</td>\n",
       "      <td>3254.000000</td>\n",
       "      <td>0.6304</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   cat          dog  accuracy    macro avg  weighted avg\n",
       "precision     0.484122     0.893177    0.6304     0.688649      0.750335\n",
       "recall        0.890607     0.490781    0.6304     0.690694      0.630400\n",
       "f1-score      0.627269     0.633479    0.6304     0.630374      0.631310\n",
       "support    1746.000000  3254.000000    0.6304  5000.000000   5000.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model 2\")\n",
    "pd.DataFrame(classification_report(paws.actual, paws.model2, \n",
    "                      labels=['cat','dog'],\n",
    "                      output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.659888</td>\n",
       "      <td>0.5096</td>\n",
       "      <td>0.509118</td>\n",
       "      <td>0.554590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.511455</td>\n",
       "      <td>0.508605</td>\n",
       "      <td>0.5096</td>\n",
       "      <td>0.510030</td>\n",
       "      <td>0.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.421425</td>\n",
       "      <td>0.574453</td>\n",
       "      <td>0.5096</td>\n",
       "      <td>0.497939</td>\n",
       "      <td>0.521016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>1746.000000</td>\n",
       "      <td>3254.000000</td>\n",
       "      <td>0.5096</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   cat          dog  accuracy    macro avg  weighted avg\n",
       "precision     0.358347     0.659888    0.5096     0.509118      0.554590\n",
       "recall        0.511455     0.508605    0.5096     0.510030      0.509600\n",
       "f1-score      0.421425     0.574453    0.5096     0.497939      0.521016\n",
       "support    1746.000000  3254.000000    0.5096  5000.000000   5000.000000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model 3\")\n",
    "pd.DataFrame(classification_report(paws.actual, paws.model3, \n",
    "                      labels=['cat','dog'],\n",
    "                      output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.731249</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.769239</td>\n",
       "      <td>0.757781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.345361</td>\n",
       "      <td>0.955747</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.650554</td>\n",
       "      <td>0.742600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.483755</td>\n",
       "      <td>0.828560</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.656157</td>\n",
       "      <td>0.708154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>1746.000000</td>\n",
       "      <td>3254.000000</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   cat          dog  accuracy    macro avg  weighted avg\n",
       "precision     0.807229     0.731249    0.7426     0.769239      0.757781\n",
       "recall        0.345361     0.955747    0.7426     0.650554      0.742600\n",
       "f1-score      0.483755     0.828560    0.7426     0.656157      0.708154\n",
       "support    1746.000000  3254.000000    0.7426  5000.000000   5000.000000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model 4\")\n",
    "pd.DataFrame(classification_report(paws.actual, paws.model4, \n",
    "                      labels=['cat','dog'],\n",
    "                      output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TP) True Positive: Guessed \"positively (yes)\" correctly\n",
    "## (TN) True Negative: Guessed \"negatively (no)\" correctly\n",
    "\n",
    "## (FP) False Positive: Guessed \"positively (yes)\" incorrectly\n",
    "## (FN) False Negative: Guessed \"negatively (no)\" incorrectly\n",
    "\n",
    "### FALSE always did it incorrectly**\n",
    "\n",
    "### TRUE always did it correctly**\n",
    "\n",
    "**Do it in two steps,**\n",
    "\n",
    "        First see if it got it right, determine TRUE or FALSE\n",
    "        Then see if it guessed yes or no, determine POSITIVE or NEGATIVE\n",
    "\n",
    "### Recall: WILLING TO ACCEPT (REDUCING FALSE NEGATIVES (FN))\n",
    "- How many the model found correctly among ALL of the cats (How often it gets it right)\n",
    "\n",
    "        TP / (TP + FN)\n",
    "\n",
    "### Precision: WILLING TO ACCEPT FALSE POSITIVES (FP)\n",
    "- Actual cats among the predicted cats of the model (How often it misses)\n",
    "\n",
    "        TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def accuracy(df):\n",
    "    baseline_recall = (subset[col] == subset['actual']).mean()\n",
    "    print(f'Baseline Recall: {baseline_recall}')\n",
    "    for col in df:\n",
    "        score = (df[col] == df['actual']).mean()\n",
    "        print(f'Accuracy for {col}: {score}')\n",
    "\n",
    "\n",
    "def recall(df, pos_label):\n",
    "    for col in df:\n",
    "        score = (df[col] == df['actual']).mean()\n",
    "        print(f'Recall for {col}: {score}')\n",
    "\n",
    "for col in df:        \n",
    "    subset = df[df.col == 'coffee']\n",
    "    model_precision = (subset.col == subset.actual).mean()\n",
    "\n",
    "    subset = df[df.col == 'coffee']\n",
    "    baseline_precision = (subset.baseline_prediction == subset.actual).mean()\n",
    "\n",
    "    print(f'Model Precision: {model_precision:.2%}')\n",
    "    print(f'Baseline Precision: {baseline_precision:.2%}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def recall(df, pos_label)\n",
    "    subset = df[df['actual'] == pos_label]\n",
    "    for col in df:\n",
    "        subset = df[df['actual'] == pos_label]\n",
    "        model_recall = (subset[col] == subset['actual']).mean()\n",
    "        baseline_recall = (subset.baseline_prediction == subset.actual).mean()\n",
    "\n",
    "print(f'   model recall: {model_recall:.2%}')\n",
    "print(f'baseline recall: {baseline_recall:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
